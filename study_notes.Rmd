---
title: "Introduction to Statistical Learning Study Notes"
author: "Orhan Aktas"
date: '2022-04-19'
output:   
    html_document:
      toc: yes
      number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  class.source = "bg-warning text-warning"
)
```

```{r libraries, message=FALSE, warning=FALSE}
library(tidyverse)
library(ISLR2)

```

# Introduction

# Statistical Learning

## What Is Statistical Learning?

### Why Estimate f?

There are two main reasons that we may wish to estimate f:

1.  prediction

2.  inference

#### Prediction

In many situations, a set of inputs X are readily available, but the output Y cannot be easily obtained. In this setting, since the error term averages to zero, we can predict Y using $\hat{Y} = \hat{f}(X)$.

#### Inference

We are often interested in understanding the association between Y and X. In this situation we wish to estimate f, but our goal is to know its exact form.

-   Which predictors are associated with the response?

-   What is the relationship between the response and each predictor?

-   Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?

#### Practical Uses

In this book, we will see a number of examples that fall into the prediction setting, the inference setting, or a combination of the two.

##### Modeling for prediction

For instance, consider a company that is interested in conducting a direct-marketing campaign. The goal is to identify individuals who are likely to respond positively to a mailing, based on observations of demographic variables measured on each individual. In this case, the demographic variables serve as predictors, and response to the marketing campaign (either positive or negative) serves as the outcome. The company is not interested in obtaining a deep understanding of the relationships between each individual predictor and the response; instead, the company simply wants to accurately predict the response using the predictors.

##### Modeling for inference

Another example involves modeling the brand of a product that a customer might purchase based on variables such as price, store location, discount levels, competition price, and so forth. In this situation one might really be most interested in the association between each variable and the probability of purchase. For instance, to what extent is the product's price associated with sales?

##### Modeling both for prediction and inference

Finally, some modeling could be conducted both for prediction and inference. For example, in a real estate setting, one may seek to relate values of homes to inputs such as crime rate, zoning, distance from a river, air quality, schools, income level of community, size of houses, and so forth. In this case one might be interested in the association between each individual input variable and housing price---for instance, how much extra will a house be worth if it has a view of the river? This is an *inference* problem. Alternatively, one may simply be interested in predicting the value of a home given its characteristics: is this house under- or over-valued? This is a *prediction* problem.

### How Do We Estimate f?

Our goal is to apply a statistical learning method to the training data in order to estimate the unknown function f. In other words, we want to find a function $\hat{f}$such that $\hat{Y} 

\sim

 \hat{f}(X)$ for any observation (X, Y). Broadly speaking, most statistical learning methods for this task can be characterized as either parametric or non-parametric. We now briefly discuss these two types of approaches.

#### Parametric methods

Parametric methods involve a two-step model-based approach.

1.  First, we make an assumption about the functional form, or shape of f.

2.  After a model has been selected, we need a procedure that uses the training data to fit or train the model. The most common approach to fitting the model is referred to as (ordinary) least squares. However, least squares is one of many possible ways to fit the linear model.

The model-based approach just described is referred to as parametric; it reduces the problem of estimating f down to one of estimating a set of parameters.

#### Non-Parametric Methods

Non-parametric methods do not make explicit assumptions about the functional form of f. Instead they seek an estimate of f that gets as close to the data points as possible without being too rough or wiggly.

major advantage over parametric approaches:

| [by avoiding the assumption of a particular functional form for f, they have the potential to accurately fit a wider range of possible shapes for f.]{.underline}

major disadvantage:

| [since they do not reduce the problem of estimating f to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for f.]{.underline}

### The Trade-Off Between Prediction Accuracy and Model Interpretability
